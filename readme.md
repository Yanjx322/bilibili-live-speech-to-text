# bilibili.com Live immediate Speech-to-Text (Chinese + English)

åŸºäºBç«™ç›´æ’­çš„å®æ—¶è¯­éŸ³å¬å†™(ä¸­è‹±åŒè¯­ï¼‰

---

## Features / åŠŸèƒ½ä»‹ç»

This project aims to help people who watch live streams but cannot hear the audio â€” such as people with hearing-issue or users in environments where sound cannot be played.  At the same time, it aims to improve the subtitle alignment speed and recognition accuracy, making it a better experience. It mainly focused on:

- ğŸ”Š **Auto Live Audio Extraction/è‡ªåŠ¨è·å–ç›´æ’­éŸ³é¢‘æµ(bç«™)**\
  Automatically fetches the audio stream from a Bilibili livestream via room ID.

- ğŸŒ **Multilingual Speech Recognition/è‹±æ±‰åŒè¯­è¯†åˆ«**\
  Supports **Chinese** and **English** recognition via Vosk offline models (and experimental HuggingFace models).

- **Integrate different models/æ•´åˆä¸åŒæ¨¡å‹**\
  Including Vosk offline models(trained) + xfyun online models + Huggingface transformers (**state-of-the-art self-supervised learning** model by Facebook AI Meta), which are listed in the [Model Zoo](#model-zoo) below.

- ğŸ› ï¸ **Developer-Friendly WebSocket API/é€šè¿‡websocketå®æ—¶è¾“å‡ºæ¥å£**\
  Exposes a standard WebSocket interface that developers can easily integrate into **web overlays, Python/Node.js services, etc.** Automatic result push over WebSocket for each recognition segment (no polling required)

- ğŸš¨ **Clean & Readable Output/è¯†åˆ«è¾“å‡ºä¼˜åŒ–**\
  Filters out unnecessary pauses, repeated tokens, and space noise for better readability.

## Example feature / ç¤ºä¾‹æ•ˆæœ



![Live Demo](./ezgif-1058bfb2b77c30.gif)



---

## ğŸ“ Requirements

- Python 3.10+
  
Install dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ“– Usage

### Step 1: Download models
if you just want to run this code provided models, please download:
 [Vosk](https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip) 
 [Vosk](https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip) 
 
 ***Please CREATE A FOLDER named `model` in the project root directory and put them inside:  
 For example:
 - `model/vosk-model-small-en-us-0.15`
 
 [HuggingFace](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn/tree/main) 
 
 ***CREATE A FOLDER 'wav2vec2-chinese' in 'model' file, and put the trained models package into it
 For example:
 - `model/wav2vec2-chinese/config.json`

or if you want the whole model zoo, you can find it below.

### Step 2: Run the main program

```bash
python main.py
```

### Step 3: Input bilibili live Room ID and Choose Model

```bash
input bilibili room number/è¯·è¾“å…¥Bilibiliç›´æ’­æˆ¿é—´å·: 1

select model/è¯·é€‰æ‹©è¯†åˆ«æ¨¡å‹ï¼š
1. Vosk(Chinese)ç¦»çº¿æ¨¡å‹
2. Vosk(English)ç¦»çº¿æ¨¡å‹
3. xfyunç§‘å¤§è®¯é£åœ¨çº¿æ¨¡å‹
4. Hugging Face wav2vec2-chinese
your input model: 3
```

### Step 4: Start Recognition

```bash
ç›´æ’­æµåœ°å€ï¼šhttps://...
å¼€å§‹è¯†åˆ« (Vosk)...
è¯†åˆ«ç»“æœ: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°ç›´æ’­é—´
è¯†åˆ«ç»“æœ: Let's begin with today's live
```

---

## Model Zoo

**1. Vosk** has provided more pretrained offline models for different languages, apart from the two given model of this project, download here:

ğŸ”— [https://alphacephei.com/vosk/models](https://alphacephei.com/vosk/models)

for immediate speech-to-text task, I chose 2 small models. Though large models provide more precision, they also reduce speed, causing a delay:

| **Example vosk modelï¼š**       |     |         |                      |
| ----------------------------- | --- | ------- | -------------------- |
| `vosk-model-small-cn-0.22`    | CHN | \~50MB  | small model          |
| `vosk-model-small-en-us-0.15` | ENG | \~40MB  |                      |
| `vosk-model-en-us-0.22`       | ENG | \~1.8GB | more precision, slow |
| `vosk-model-cn-0.22`          | CHN | \~1.8GB |                      |

---

**2. HuggingFace** **Wav2Vec2** is a state-of-the-art **self-supervised speech representation learning model** developed by Facebook AI (Meta). It is trained on raw unlabeled audio and fine-tuned on transcribed speech for tasks like automatic speech recognition (ASR).

Chinese pretrained: [https://huggingface.co/urarik/chinese-wav2vec2-large-CV16](https://huggingface.co/urarik/chinese-wav2vec2-large-CV16)\
English pretrained: [https://huggingface.co/facebook/wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h)

---

**3. xfyunï¼ˆç§‘å¤§è®¯é£ï¼‰Online Speech Recognition**\
xfyun is a powerful cloud-based ASR engine developed by iFLYTEK (ç§‘å¤§è®¯é£), providing real-time speech-to-text conversion with high accuracy for both Chinese and English.\
in this project, since it is online, we don't need any models downloaded, just run the main.py and choose it.
Websiteï¼š[https://www.xfyun.cn/services/online\_asr](https://www.xfyun.cn/services/online_asr)

A wider usage requires registering an API key on the iFLYTEK open platform, and this project already includes basic integration logic in `xf_recognizer.py`.

---

## ğŸ“‚ Directory Structure

```
project/
â”œâ”€â”€ main.py
â”œâ”€â”€ bilibili_stream.py
â”œâ”€â”€ vosk_recognizer.py
â”œâ”€â”€ vosk_recognizer_English.py
â”œâ”€â”€ model_loader.py
â”œâ”€â”€ xf_recognizer.py
â”œâ”€â”€ hf_recognizer.py
â”œâ”€â”€ requirement.txt
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ vosk-model-small-cn-0.22/
â”‚   â””â”€â”€ vosk-model-small-en-us-0.15/
|    ......
```

---

## ğŸ™ Credits

- [Vosk Speech Recognition](https://alphacephei.com/vosk/)
- [HuggingFace Transformers](https://huggingface.co/models)
- Bilibili Open Platform
